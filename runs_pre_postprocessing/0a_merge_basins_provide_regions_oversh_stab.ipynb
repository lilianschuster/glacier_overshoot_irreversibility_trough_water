{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be47a8d5-23db-4801-a0b7-98997530b38a",
   "metadata": {},
   "source": [
    "# 0a: Combine all used glacier variables into merged data-files for every RGI or PROVIDE region \n",
    "uses the raw data to get the glacier volume, area, and to compute runoff and meltwater components on a monthly and annual basis\n",
    "- uses the RGI batch files (e.g. `https://cluster.klima.uni-bremen.de/~lschuster/provide/gfdl-esm2m_oversh_stab_uni_bern/runs/output/RGI03/run_hydro_w5e5_gcm_merged_from_2000_gfdl-esm2m_oversh_T20OS15_endyr_2500_bc_1980_2019_rgi03_0_1000.nc`) as input \n",
    "- creates merged data-files in these folders that are later further aggregated  \n",
    "    - https://cluster.klima.uni-bremen.de/~lschuster/provide/gfdl-esm2m_oversh_stab_uni_bern/runs/output/rgi_reg\n",
    "    - https://cluster.klima.uni-bremen.de/~lschuster/provide/gfdl-esm2m_oversh_stab_uni_bern/runs/output/provide_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a716fe-137d-4421-b4ea-0b518ea06747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "import oggm.cfg\n",
    "from oggm import  utils, workflow, tasks, graphics\n",
    "import json\n",
    "import geopandas as gpd\n",
    "\n",
    "# let's also do some visualisations ...\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "scenarios = ['stab_T12','stab_T15','oversh_T20OS15','oversh_T25OS15',\n",
    "             'oversh_T30OS15','stab_T20','stab_T25','stab_T30']\n",
    "\n",
    "# get the dataset where coordinates of glaciers are stored\n",
    "frgi = utils.file_downloader('https://cluster.klima.uni-bremen.de/~oggm/rgi/rgi62_stats.h5')\n",
    "#frgi = '/home/users/lschuster/glacierMIP/rgi62_stats.h5'\n",
    "odf = pd.read_hdf(frgi, index_col=0)\n",
    "# choose only relevant variables\n",
    "variables = ['volume','area', #'length',\n",
    "             'melt_off_glacier', 'melt_on_glacier', 'liq_prcp_off_glacier', 'liq_prcp_on_glacier',\n",
    "             'melt_off_glacier_monthly', 'melt_on_glacier_monthly', 'liq_prcp_off_glacier_monthly', 'liq_prcp_on_glacier_monthly']\n",
    "             #'on_area', 'off_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a38ba4-eb98-48a2-bff6-806813b24592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "681a3edb-ea6d-433d-bbba-227aad1433bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### For Provide-regions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b58137-04f2-49d8-b3f2-c7d5d5fcad83",
   "metadata": {},
   "source": [
    "first create a summary table about the Provide-regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de268ebd-d63c-4330-9418-1a43666c60c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216502\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b57ac th {\n",
       "  text-align: center;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b57ac\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b57ac_level0_col0\" class=\"col_heading level0 col0\" >n glaciers</th>\n",
       "      <th id=\"T_b57ac_level0_col1\" class=\"col_heading level0 col1\" >Initial volume relative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row0\" class=\"row_heading level0 row0\" >P01</th>\n",
       "      <td id=\"T_b57ac_row0_col0\" class=\"data row0 col0\" >45942</td>\n",
       "      <td id=\"T_b57ac_row0_col1\" class=\"data row0 col1\" >12.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row1\" class=\"row_heading level0 row1\" >P02</th>\n",
       "      <td id=\"T_b57ac_row1_col0\" class=\"data row1 col0\" >11971</td>\n",
       "      <td id=\"T_b57ac_row1_col1\" class=\"data row1 col1\" >23.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row2\" class=\"row_heading level0 row2\" >P03</th>\n",
       "      <td id=\"T_b57ac_row2_col0\" class=\"data row2 col0\" >19306</td>\n",
       "      <td id=\"T_b57ac_row2_col1\" class=\"data row2 col1\" >9.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row3\" class=\"row_heading level0 row3\" >P04</th>\n",
       "      <td id=\"T_b57ac_row3_col0\" class=\"data row3 col0\" >4033</td>\n",
       "      <td id=\"T_b57ac_row3_col1\" class=\"data row3 col1\" >2.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row4\" class=\"row_heading level0 row4\" >P05</th>\n",
       "      <td id=\"T_b57ac_row4_col0\" class=\"data row4 col0\" >2840</td>\n",
       "      <td id=\"T_b57ac_row4_col1\" class=\"data row4 col1\" >13.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row5\" class=\"row_heading level0 row5\" >P06</th>\n",
       "      <td id=\"T_b57ac_row5_col0\" class=\"data row5 col0\" >2218</td>\n",
       "      <td id=\"T_b57ac_row5_col1\" class=\"data row5 col1\" >0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row6\" class=\"row_heading level0 row6\" >P07</th>\n",
       "      <td id=\"T_b57ac_row6_col0\" class=\"data row6 col0\" >3927</td>\n",
       "      <td id=\"T_b57ac_row6_col1\" class=\"data row6 col1\" >0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row7\" class=\"row_heading level0 row7\" >P08</th>\n",
       "      <td id=\"T_b57ac_row7_col0\" class=\"data row7 col0\" >1888</td>\n",
       "      <td id=\"T_b57ac_row7_col1\" class=\"data row7 col1\" >0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row8\" class=\"row_heading level0 row8\" >P09</th>\n",
       "      <td id=\"T_b57ac_row8_col0\" class=\"data row8 col0\" >98286</td>\n",
       "      <td id=\"T_b57ac_row8_col1\" class=\"data row8 col1\" >4.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row9\" class=\"row_heading level0 row9\" >P10</th>\n",
       "      <td id=\"T_b57ac_row9_col0\" class=\"data row9 col0\" >2898</td>\n",
       "      <td id=\"T_b57ac_row9_col1\" class=\"data row9 col1\" >0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row10\" class=\"row_heading level0 row10\" >P11</th>\n",
       "      <td id=\"T_b57ac_row10_col0\" class=\"data row10 col0\" >15908</td>\n",
       "      <td id=\"T_b57ac_row10_col1\" class=\"data row10 col1\" >3.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row11\" class=\"row_heading level0 row11\" >P12</th>\n",
       "      <td id=\"T_b57ac_row11_col0\" class=\"data row11 col0\" >3537</td>\n",
       "      <td id=\"T_b57ac_row11_col1\" class=\"data row11 col1\" >0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b57ac_level0_row12\" class=\"row_heading level0 row12\" >P13</th>\n",
       "      <td id=\"T_b57ac_row12_col0\" class=\"data row12 col0\" >2752</td>\n",
       "      <td id=\"T_b57ac_row12_col1\" class=\"data row12 col1\" >29.380000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9952267c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_itmix = pd.read_hdf(oggm.utils.get_demo_file('rgi62_itmix_df.h5'))\n",
    "print(len(df_itmix))\n",
    "\n",
    "preg_list = []\n",
    "for j, pi in enumerate(np.arange(1,14,1)):\n",
    "    if pi<10:\n",
    "        Preg = f'P0{pi}'\n",
    "    else:\n",
    "        Preg = f'P{pi}'\n",
    "    preg_list.append(Preg)\n",
    "    \n",
    "pd_provide_reg = pd.DataFrame(index=preg_list)\n",
    "\n",
    "rgi_ids_per_provide_region_dict = {}\n",
    "for pi in np.arange(1,14,1):\n",
    "    f = open('/home/www/lschuster/provide/provide_glacier_regions/rgi_ids_per_provide_region.json')\n",
    "    if pi<10:\n",
    "        Preg = f'P0{pi}'\n",
    "    else:\n",
    "        Preg = f'P{pi}'\n",
    "    rgis_basin = json.load(f)[Preg]\n",
    "    # remove connectivity level 2 glaciers only important for P03 (Greenland)\n",
    "    rgis_basin = odf.loc[rgis_basin].loc[odf.loc[rgis_basin].Connect <2].index.values\n",
    "    rgis_basin = list(set(rgis_basin))\n",
    "    \n",
    "    pd_provide_reg.loc[Preg,'n glaciers'] = int(len(rgis_basin))\n",
    "    vol_ratio = 100*df_itmix.loc[rgis_basin]['vol_itmix_m3'].sum()/df_itmix['vol_itmix_m3'].sum()\n",
    "    pd_provide_reg.loc[Preg,'Initial volume relative'] = vol_ratio\n",
    "    rgi_ids_per_provide_region_dict[Preg] = rgis_basin\n",
    "pd_provide_reg['Initial volume relative'] = pd_provide_reg['Initial volume relative'].round(2).values\n",
    "pd_provide_reg['n glaciers'] = pd_provide_reg['n glaciers'].astype(int)\n",
    "\n",
    "import dataframe_image as dfi\n",
    "dfi.export(pd_provide_reg, \"/home/www/lschuster/provide/gfdl-esm2m_oversh_stab_uni_bern/analysis_notebooks/table.png\", table_conversion='matplotlib')\n",
    "\n",
    "from IPython.display import HTML\n",
    "styles = [\n",
    "    dict(selector=\"th\", props=[(\"text-align\", \"center\")])]\n",
    "pd_provide_reg.style.set_table_styles(styles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abefba96-4f69-4e61-b148-f5c9ece85533",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_geodetic = utils.get_geodetic_mb_dataframe()[utils.get_geodetic_mb_dataframe().period=='2000-01-01_2020-01-01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24086d1f-d35b-4b11-a096-dc61546d4c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215547"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd_geodetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a16d666d-c440-4ee8-babe-95b93be19c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215506"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_provide_reg['n glaciers'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be7a6d7-be46-4a00-b568-6b95f532bb7d",
   "metadata": {},
   "source": [
    "### now create the merged per provide region files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa18edc4-3ffa-49f5-9803-94e7e0970b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## this produces much smaller files, but we want to still have the monthly runoff \n",
    "# variables_sel = ['volume','melt_off_glacier',\n",
    "# 'melt_on_glacier',\n",
    "# 'liq_prcp_off_glacier',\n",
    "# 'liq_prcp_on_glacier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2962d572-63e9-482f-b1f2-114e7e4afb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 935 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 935 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 935 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 935 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 935 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n",
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 940 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/lschuster/.local/lib/python3.10/site-packages/xarray/core/indexing.py:1228: PerformanceWarning: Slicing with an out-of-order index is generating 935 times more chunks\n",
      "  return self.array[key]\n"
     ]
    }
   ],
   "source": [
    "creation_date = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())  # here add the current time for info\n",
    "path = f'/home/www/lschuster/provide/gfdl-esm2m_oversh_stab_uni_bern/runs/output'\n",
    "\n",
    "bc = '_bc_1980_2019' #'_bc_1980_2019' #'_bc_2000_2019' #'_bc_1980_2019' #'_bc_2000_2019' # '_bc_1979_2014'\n",
    "merge =  True\n",
    "if merge:\n",
    "    for Preg in list(rgi_ids_per_provide_region_dict.keys()):\n",
    "        rgis_preg = rgi_ids_per_provide_region_dict[Preg]\n",
    "        odf_basin = odf.loc[rgis_preg]\n",
    "        #len(rgis_preg)\n",
    "    \n",
    "        for hist in ['w5e5_gcm_merged_from_2000']:\n",
    "            utils.mkdir(f'{path}/provide_reg/{Preg}')\n",
    "\n",
    "            #ds_all = []  # in this array all datasets going to be stored with additional coordinates GCM and SCENARIO\n",
    "            #for GCM in all_GCM:  # loop through all GCMs\n",
    "            for scen in scenarios:  # loop through all SSPs\n",
    "                ds_allt = []\n",
    "                for rgi_reg in odf_basin.O1Region.unique():\n",
    "                    print(rgi_reg)\n",
    "                    #rid = '_{}_{}'.format(GCM, scen)  # put together the same filesuffix which was used during the projection runs\n",
    "                \n",
    "                    odf_basin_reg = odf_basin.loc[odf_basin.O1Region==rgi_reg]\n",
    "                    rgis_basin_rgi_reg = odf_basin_reg.index.values\n",
    "                    #run_hydro_w5e5_gcm_merged_from_2000_gfdl-esm2m_oversh_T20OS15_endyr_2500_bc_1980_2019_rgi01_0_1000.nc\n",
    "                    with xr.open_mfdataset(f'{path}/RGI{rgi_reg}/run_hydro_{hist}_gfdl-esm2m_{scen}_endyr_2500{bc}_rgi{rgi_reg}_*.nc') as ds_tmp:\n",
    "                        ds_tmp = ds_tmp[variables]\n",
    "                        ds_tmp = ds_tmp.sel(rgi_id=rgis_basin_rgi_reg).load()\n",
    "\n",
    "                        ds_tmp['runoff_monthly'] = (ds_tmp['melt_off_glacier_monthly'] + ds_tmp['melt_on_glacier_monthly']\n",
    "                                                    + ds_tmp['liq_prcp_off_glacier_monthly'] + ds_tmp['liq_prcp_on_glacier_monthly'])\n",
    "                        ds_tmp['melt_off_on_monthly'] = (ds_tmp['melt_off_glacier_monthly'] + ds_tmp['melt_on_glacier_monthly'])\n",
    "                        ds_tmp['runoff'] = (ds_tmp['melt_off_glacier'] + ds_tmp['melt_on_glacier']\n",
    "                                                    + ds_tmp['liq_prcp_off_glacier'] + ds_tmp['liq_prcp_on_glacier'])\n",
    "                        ds_tmp['melt_off_on'] = (ds_tmp['melt_off_glacier'] + ds_tmp['melt_on_glacier'])\n",
    "                        # for the moment, only save these variables ...\n",
    "                        ds_tmp = ds_tmp[['runoff', 'melt_off_on','volume', 'area', 'runoff_monthly', 'melt_off_on_monthly']]\n",
    "                    ds_tmp.coords['gcm'] = 'GFDL-ESM2M'  # add GCM as a coordinate\n",
    "                    ds_tmp.coords['gcm'].attrs['description'] = 'used global circulation model'  # add a description for GCM\n",
    "                    ds_tmp = ds_tmp.expand_dims(\"gcm\")  # add GCM as a dimension to all Data variables\n",
    "                    ds_tmp.coords['scenario'] = scen  # add scenario (here ssp) as a coordinate\n",
    "                    ds_tmp.coords['scenario'].attrs['description'] = 'used scenario (here overshoot, stabilisation or commitment scenarios)'\n",
    "                    ds_tmp.coords['bias_correction'] = bc[1:]\n",
    "                    ds_tmp.coords['provide_region'] = Preg\n",
    "                    ds_tmp.coords['OGGM_version'] = 'OGGM_v161_gdirs_2023.3'\n",
    "                    ds_tmp = ds_tmp.expand_dims(\"scenario\")  # add SSO as a dimension to all Data variables\n",
    "                    ds_tmp.attrs['creation_date'] = creation_date  # also add todays date for info\n",
    "                    ds_tmp.runoff.attrs = {'unit':'kg yr-1', \n",
    "                                           'description':'Annual glacier runoff: sum of annual melt and liquid precipitation on and off the glacier using a fixed-gauge with a glacier minimum reference area from year 2000'}\n",
    "                    ds_tmp.runoff_monthly.attrs = {'unit':'kg month-1',\n",
    "                                                   'description':'Monthly glacier runoff from sum of monthly melt and liquid precipitation on and off the glacier using a fixed-gauge with a glacier minimum reference area from year 2000'}\n",
    "                    ds_tmp.melt_off_on_monthly.attrs = {'unit':'kg yr-1', \n",
    "                                                       'description':'Monthly meltwater components from glacier runoff: sum of meltwater on and off the glacier using a fixed-gauge with a glacier minimum reference area from year 2000'}\n",
    "                    ds_tmp.melt_off_on.attrs = {'unit':'kg yr-1', \n",
    "                                                       'description':'Annual meltwater components from glacier runoff: sum of meltwater on and off the glacier using a fixed-gauge with a glacier minimum reference area from year 2000'}\n",
    "                    ds_allt.append(ds_tmp)  # add the dataset with extra coordinates to our final ds_all array\n",
    "                #ds_tmp_s = xr.concat(ds_allt, dim='scenario', fill_value=np.NaN)\n",
    "                #ds_all.append(ds_tmp_s)\n",
    "\n",
    "                # do the actual merging\n",
    "                ds_merged = xr.concat(ds_allt, fill_value=np.nan, dim='rgi_id')\n",
    "                #ds_merged = xr.combine_by_coords(ds_all, fill_value=np.nan)  # define how the missing GCM, SCENARIO combinations should be filled  \n",
    "                #ds_merged.runoff.attrs = {'unit':'kg yr-1',\n",
    "                #             'description':'Annual glacier runoff: sum of annual melt and liquid precipitation on and off the glacier, minimum reference area: 2000'}\n",
    "                #ds_merged.runoff_monthly.attrs = {'unit':'kg month-1',\n",
    "                #             'description':'Monthly glacier runoff: sum of monthly melt and liquid precipitation on and off the glacier, minimum reference area: 2000'}\n",
    "                ds_merged.to_netcdf(f'{path}/provide_reg/{Preg}/provide_reg_{Preg}_{scen}_run_hydro_{hist}_endyr2500{bc}.nc')\n",
    "                #ds_merged.to_netcdf(f'{path}/basins/basin_{basin_idx}_combined_run_hydro_{hist}_endyr2100_CMIP6{bc}.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d4b39-b927-4be1-ad53-380a640e5ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09446c59-5f8d-4261-b90c-e3d2a4aec5bd",
   "metadata": {},
   "source": [
    "### Now merge the random climate timeseries together for every Provide region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7022a-524e-47e3-8e94-e0598e5bc13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['volume']\n",
    "#'length',\n",
    "\n",
    "creation_date = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())  # here add the current time for info\n",
    "path = f'/home/www/lschuster/provide/gfdl-esm2m_oversh_stab_uni_bern/runs/output'\n",
    "\n",
    "bc = '_bc_1980_2019' #'_bc_1980_2019' #'_bc_2000_2019' #'_bc_1980_2019' #'_bc_2000_2019' # '_bc_1979_2014'\n",
    "merge =  True\n",
    "if merge:\n",
    "    for Preg in list(rgi_ids_per_provide_region_dict.keys()):\n",
    "        rgis_preg = rgi_ids_per_provide_region_dict[Preg]\n",
    "        odf_basin = odf.loc[rgis_preg]\n",
    "        for hist in ['w5e5_gcm_merged_from_2000']:\n",
    "            #utils.mkdir(f'{path}/provide_reg/{hist}{bc}')\n",
    "            #utils.mkdir(f'{path}/provide_reg/{hist}{bc}/{Preg}')\n",
    "\n",
    "            #ds_all = []  # in this array all datasets going to be stored with additional coordinates GCM and SCENARIO\n",
    "            #for GCM in all_GCM:  # loop through all GCMs\n",
    "            for scen in ['oversh_T30OS15','stab_T15','zero']:  # loop through all SSPs\n",
    "                print(Preg, scen)\n",
    "                ds_allt = []\n",
    "                for rgi_reg in odf_basin.O1Region.unique():\n",
    "                    #rid = '_{}_{}'.format(GCM, scen)  # put together the same filesuffix which was used during the projection runs\n",
    "                \n",
    "                    odf_basin_reg = odf_basin.loc[odf_basin.O1Region==rgi_reg]\n",
    "                    rgis_basin_rgi_reg = odf_basin_reg.index.values\n",
    "                    #run_hydro_w5e5_gcm_merged_from_2000_gfdl-esm2m_oversh_T20OS15_endyr_2500_bc_1980_2019_rgi01_0_1000.nc\n",
    "                    with xr.open_mfdataset(f'{path}/RGI{rgi_reg}/run_random_climate_from2500_using2400_2500_gfdl-esm2m_stab_T15_initial_{scen}{bc}_rgi{rgi_reg}_*.nc') as ds_tmp:\n",
    "                        ds_tmp = ds_tmp[variables] #.sel(time=slice(0,101*100))\n",
    "                        ds_tmp = ds_tmp.sel(rgi_id=rgis_basin_rgi_reg).load()\n",
    "\n",
    "                    ds_tmp.coords['gcm'] = 'GFDL-ESM2M repeated 2399-2499 climate'  # add GCM as a coordinate\n",
    "                    ds_tmp.coords['gcm'].attrs['description'] = 'used global circulation model'  # add a description for GCM\n",
    "                    ds_tmp = ds_tmp.expand_dims(\"gcm\")  # add GCM as a dimension to all Data variables\n",
    "                    ds_tmp.coords['scenario'] = f'initial state: {scen} after 500 years'  # add scenario (here ssp) as a coordinate\n",
    "                    ds_tmp.coords['scenario'].attrs['description'] = '2399-2499 stab_T15 used as GCM, initial state from the chosen scenario after 500 years'\n",
    "                    ds_tmp.coords['bias_correction'] = bc[1:]\n",
    "                    ds_tmp.coords['provide_region'] = Preg\n",
    "                    ds_tmp.coords['OGGM_version'] = 'OGGM_v161_gdirs_2023.3'\n",
    "                    ds_tmp = ds_tmp.expand_dims(\"scenario\")  # add SSO as a dimension to all Data variables\n",
    "                    ds_tmp.attrs['creation_date'] = creation_date  # also add todays date for info\n",
    "                    ds_allt.append(ds_tmp)  # add the dataset with extra coordinates to our final ds_all array\n",
    "                #ds_tmp_s = xr.concat(ds_allt, dim='scenario', fill_value=np.NaN)\n",
    "                #ds_all.append(ds_tmp_s)\n",
    "\n",
    "                # do the actual merging\n",
    "                ds_merged = xr.concat(ds_allt, fill_value=np.nan, dim='rgi_id')\n",
    "                #ds_merged = xr.combine_by_coords(ds_all, fill_value=np.nan)  # define how the missing GCM, SCENARIO combinations should be filled  \n",
    "                #ds_merged.runoff.attrs = {'unit':'kg yr-1',\n",
    "                #             'description':'Annual glacier runoff: sum of annual melt and liquid precipitation on and off the glacier, minimum reference area: 2000'}\n",
    "                #ds_merged.runoff_monthly.attrs = {'unit':'kg month-1',\n",
    "                #             'description':'Monthly glacier runoff: sum of monthly melt and liquid precipitation on and off the glacier, minimum reference area: 2000'}\n",
    "                ds_merged.to_netcdf(f'{path}/provide_reg/{Preg}/random_climate_from2500_using2399_2499_provide_reg_{Preg}_initial_{scen}_{hist}{bc}.nc')\n",
    "                #ds_merged.to_netcdf(f'{path}/basins/basin_{basin_idx}_combined_run_hydro_{hist}_endyr2100_CMIP6{bc}.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b538e080-ebab-4948-bf96-0ca9c98c81cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c1fe993-28c8-4fa1-ab41-fb1b20964ea1",
   "metadata": {},
   "source": [
    "### repeat the same for RGI regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d893d733-095e-424f-aa86-12a4c6af2104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n",
      "02\n",
      "03\n",
      "04\n",
      "05\n",
      "06\n",
      "07\n",
      "08\n",
      "09\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "creation_date = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())  # here add the current time for info\n",
    "path = f'/home/www/lschuster/provide/gfdl-esm2m_oversh_stab_uni_bern/runs/output'\n",
    "odf_s = odf.loc[(odf['Connect'] == 0) | (odf['Connect'] ==1)]\n",
    "variables_s = ['volume', 'area', 'melt_off_glacier', 'melt_on_glacier', 'liq_prcp_off_glacier', 'liq_prcp_on_glacier']\n",
    "# this was run once for '_bc_1980_2019' and once for '_bc_2000_2019' \n",
    "bc = '_bc_1980_2019'\n",
    "merge =  True\n",
    "if merge:\n",
    "    for rgi_reg in list(rgi_regs):\n",
    "        rgi_ids_region = odf_s.loc[odf['O1Region'] == rgi_reg].index.values\n",
    "        print(rgi_reg)\n",
    "        for hist in ['w5e5_gcm_merged_from_2000']:\n",
    "            utils.mkdir(f'{path}/rgi_reg/{rgi_reg}')\n",
    "            #for GCM in all_GCM:  # loop through all GCMs\n",
    "            for scen in scenarios:  # loop through all SSPs\n",
    "                ds_allt = []   \n",
    "                with xr.open_mfdataset(f'{path}/RGI{rgi_reg}/run_hydro_{hist}_gfdl-esm2m_{scen}_endyr_2500{bc}_rgi{rgi_reg}_*.nc') as ds_tmp:\n",
    "                    ds_tmp = ds_tmp[variables_s]\n",
    "                    ds_tmp = ds_tmp.sel(rgi_id=rgi_ids_region).load()\n",
    "                    # for the RGI regions, we don't need monthly runoff stuff \n",
    "                    #ds_tmp['runoff_monthly'] = (ds_tmp['melt_off_glacier_monthly'] + ds_tmp['melt_on_glacier_monthly']\n",
    "                    #                            + ds_tmp['liq_prcp_off_glacier_monthly'] + ds_tmp['liq_prcp_on_glacier_monthly'])\n",
    "                    #ds_tmp['melt_off_on_monthly'] = (ds_tmp['melt_off_glacier_monthly'] + ds_tmp['melt_on_glacier_monthly'])\n",
    "                    ds_tmp['runoff'] = (ds_tmp['melt_off_glacier'] + ds_tmp['melt_on_glacier']\n",
    "                                                + ds_tmp['liq_prcp_off_glacier'] + ds_tmp['liq_prcp_on_glacier'])\n",
    "                    ds_tmp['melt_off_on'] = (ds_tmp['melt_off_glacier'] + ds_tmp['melt_on_glacier'])\n",
    "                    # for the moment, only save these variables ...\n",
    "                    ds_tmp = ds_tmp[['runoff', 'melt_off_on','volume', 'area']] #, 'runoff_monthly', 'melt_off_on_monthly']]\n",
    "                ds_tmp.coords['gcm'] = 'GFDL-ESM2M'  # add GCM as a coordinate\n",
    "                ds_tmp.coords['gcm'].attrs['description'] = 'used global circulation model'  # add a description for GCM\n",
    "                ds_tmp = ds_tmp.expand_dims(\"gcm\")  # add GCM as a dimension to all Data variables\n",
    "                ds_tmp.coords['scenario'] = scen  # add scenario (here ssp) as a coordinate\n",
    "                ds_tmp.coords['scenario'].attrs['description'] = 'used scenario (here overshoot, stabilisation or commitment scenarios)'\n",
    "                ds_tmp.coords['bias_correction'] = bc[1:]\n",
    "                ds_tmp.coords['OGGM_version'] = 'OGGM_v161_gdirs_2023.3'\n",
    "                ds_tmp = ds_tmp.expand_dims(\"scenario\")  # add SSO as a dimension to all Data variables\n",
    "                ds_tmp.attrs['creation_date'] = creation_date  # also add todays date for info\n",
    "                ds_tmp.runoff.attrs = {'unit':'kg yr-1', \n",
    "                                       'description':'Annual glacier runoff: sum of annual melt and liquid precipitation on and off the glacier using a fixed-gauge with a glacier minimum reference area from year 2000'}\n",
    "                ds_tmp.melt_off_on.attrs = {'unit':'kg yr-1', \n",
    "                                            'description':'Annual meltwater components from glacier runoff: sum of meltwater on and off the glacier using a fixed-gauge with a glacier minimum reference area from year 2000'}\n",
    "                #ds_tmp.runoff_monthly.attrs = {'unit':'kg month-1',\n",
    "                #                               'description':'Monthly glacier runoff from sum of monthly melt and liquid precipitation on and off the glacier using a fixed-gauge with a glacier minimum reference area from year 2000'}\n",
    "                ds_tmp.to_netcdf(f'{path}/rgi_reg/{rgi_reg}/rgi_reg_{rgi_reg}_{scen}_run_hydro_{hist}_endyr2500{bc}.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c2717-a753-49fa-8a42-091ef459244b",
   "metadata": {},
   "source": [
    "**also merge the random climate timeseries into RGI regions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392493c-8e4e-4b1e-9d7e-24327c0ae616",
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_date = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())  # here add the current time for info\n",
    "path = f'/home/www/lschuster/provide/gfdl-esm2m_oversh_stab_uni_bern/runs/output'\n",
    "odf_s = odf.loc[(odf['Connect'] == 0) | (odf['Connect'] ==1)]\n",
    "variables_r = ['volume']\n",
    "\n",
    "bc = '_bc_1980_2019' #'_bc_1980_2019' #'_bc_2000_2019' #'_bc_1980_2019' #'_bc_2000_2019' # '_bc_1979_2014'\n",
    "merge =  True\n",
    "if merge:\n",
    "    for rgi_reg in list(rgi_regs):\n",
    "        rgi_ids_region = odf_s.loc[odf['O1Region'] == rgi_reg].index.values\n",
    "        print(rgi_reg)\n",
    "        for hist in ['w5e5_gcm_merged_from_2000']:\n",
    "            #for GCM in all_GCM:  # loop through all GCMs\n",
    "            for scen in ['oversh_T30OS15','stab_T15','zero']:  # loop through all SSPs\n",
    "                ds_allt = []\n",
    "                with xr.open_mfdataset(f'{path}/RGI{rgi_reg}/run_random_climate_from2500_using2400_2500_gfdl-esm2m_stab_T15_initial_{scen}{bc}_rgi{rgi_reg}_*.nc') as ds_tmp:\n",
    "                    ds_tmp = ds_tmp[variables_r]\n",
    "                    ds_tmp = ds_tmp.sel(rgi_id=rgi_ids_region).load()\n",
    "                    \n",
    "                ds_tmp.coords['gcm'] = 'GFDL-ESM2M'  # add GCM as a coordinate\n",
    "                ds_tmp.coords['gcm'].attrs['description'] = 'used global circulation model'  # add a description for GCM\n",
    "                ds_tmp = ds_tmp.expand_dims(\"gcm\")  # add GCM as a dimension to all Data variables\n",
    "                ds_tmp.coords['scenario'] = f'initial state: {scen} after 500 years'  # add scenario (here ssp) as a coordinate\n",
    "                ds_tmp.coords['scenario'].attrs['description'] = '2399-2499 stab_T15 used as GCM, initial state from the chosen scenario after 500 years'\n",
    "                ds_tmp.coords['bias_correction'] = bc[1:]\n",
    "                ds_tmp.coords['OGGM_version'] = 'OGGM_v161_gdirs_2023.3'\n",
    "                ds_tmp = ds_tmp.expand_dims(\"scenario\")  # add SSO as a dimension to all Data variables\n",
    "                ds_tmp.attrs['creation_date'] = creation_date  # also add todays date for info\n",
    "                ds_tmp.to_netcdf(f'{path}/rgi_reg/{rgi_reg}/random_climate_from2500_using2399_2499_rgi_reg_{rgi_reg}_initial_{scen}_{hist}{bc}.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a42bd-76fd-433c-b449-95d606697b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "308b1701-48d7-4e08-81ce-3e70c6d590d3",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b7b4f-c453-449f-bc44-b0bd0d88789f",
   "metadata": {},
   "source": [
    "- we don't need separate files for the basins, because we will do the filling first (2_fill_misssing_glaciers.ipynb), then we add a basin_id to directly select the wished basin ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9004ea8a-02b4-4e04-9432-63090f96f6d6",
   "metadata": {},
   "source": [
    "### For basins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd066c6-2ea8-4bec-b042-af475b2553e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "creation_date = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())  # here add the current time for info\n",
    "path = f'/home/www/lschuster/provide/gfdl-esm2m_oversh_stab_uni_bern/runs/output'\n",
    "pd_basin_num = gpd.read_file('/home/www/fmaussion/misc/magicc/basins_shape/glacier_basins.shp')\n",
    "\n",
    "bc = '_bc_1980_2019' #'_bc_2000_2019' # '_bc_1979_2014'\n",
    "merge = False #True\n",
    "if merge:\n",
    "    for basin in pd_basin_num.RIVER_BASI:\n",
    "        # get the RGI ids of the glaciers of the chosen basin\n",
    "        basin_idx = pd_basin_num[pd_basin_num['RIVER_BASI'] == basin]['MRBID'].values[0]\n",
    "        f = open('/home/www/fmaussion/misc/magicc/rgi_ids_per_basin.json')\n",
    "        rgis_basin = json.load(f)[str(basin_idx)]\n",
    "        odf_basin = odf.loc[rgis_basin]\n",
    "        len(rgis_basin)\n",
    "    \n",
    "        for hist in ['w5e5_gcm_merged_from_2000']:\n",
    "            utils.mkdir(f'{path}/basins/{hist}{bc}')\n",
    "            utils.mkdir(f'{path}/basins/{hist}{bc}/{basin_idx}')\n",
    "\n",
    "            ds_all = []  # in this array all datasets going to be stored with additional coordinates GCM and SCENARIO\n",
    "            #for GCM in all_GCM:  # loop through all GCMs\n",
    "            for scen in scenarios:  # loop through all SSPs\n",
    "                try:  # check if GCM, SCENARIO combination exists\n",
    "                    #rid = '_{}_{}'.format(GCM, scen)  # put together the same filesuffix which was used during the projection runs\n",
    "                    for rgi_reg in odf_basin.O1Region.unique():\n",
    "                        odf_basin_reg = odf_basin.loc[odf_basin.O1Region==rgi_reg]\n",
    "                        rgis_basin_rgi_reg = odf_basin_reg.index.values\n",
    "                        #run_hydro_w5e5_gcm_merged_from_2000_gfdl-esm2m_oversh_T20OS15_endyr_2500_bc_1980_2019_rgi01_0_1000.nc\n",
    "                        with xr.open_mfdataset(f'{path}/RGI{rgi_reg}/run_hydro_{hist}_gfdl-esm2m_{scen}_endyr_2500{bc}_rgi{rgi_reg}_*.nc') as ds_tmp:\n",
    "                            ds_tmp = ds_tmp[variables]\n",
    "                            ds_tmp = ds_tmp.sel(rgi_id=rgis_basin_rgi_reg).load()\n",
    "                            ds_tmp['runoff_monthly'] = (ds_tmp['melt_off_glacier_monthly'] + ds_tmp['melt_on_glacier_monthly']\n",
    "                                                        + ds_tmp['liq_prcp_off_glacier_monthly'] + ds_tmp['liq_prcp_on_glacier_monthly'])\n",
    "                            ds_tmp['runoff'] = (ds_tmp['melt_off_glacier'] + ds_tmp['melt_on_glacier']\n",
    "                                                        + ds_tmp['liq_prcp_off_glacier'] + ds_tmp['liq_prcp_on_glacier'])\n",
    "                            # for the moment, only save these variables ...\n",
    "                            ds_tmp = ds_tmp[['runoff', 'runoff_monthly', 'volume', 'area']]\n",
    "                        ds_tmp.coords['gcm'] = 'GFDL-ESM2M'  # add GCM as a coordinate\n",
    "                        ds_tmp.coords['gcm'].attrs['description'] = 'used global circulation model'  # add a description for GCM\n",
    "                        ds_tmp = ds_tmp.expand_dims(\"gcm\")  # add GCM as a dimension to all Data variables\n",
    "                        ds_tmp.coords['scenario'] = scen  # add scenario (here ssp) as a coordinate\n",
    "                        ds_tmp.coords['scenario'].attrs['description'] = 'used scenario (here idealized overshoot and stabilisation scenarios)'\n",
    "                        ds_tmp.coords['bias_correction'] = bc[1:]\n",
    "                        ds_tmp = ds_tmp.expand_dims(\"scenario\")  # add SSO as a dimension to all Data variables\n",
    "                        ds_tmp.attrs['creation_date'] = creation_date  # also add todays date for info\n",
    "                        ds_tmp.runoff.attrs = {'unit':'kg yr-1', \n",
    "                                               'description':'Annual glacier runoff: sum of annual melt and liquid precipitation on and off the glacier using a fixed-gauge with a glacier minimum reference area from year 2000'}\n",
    "                        ds_tmp.runoff_monthly.attrs = {'unit':'kg month-1',\n",
    "                                                       'description':'Monthly glacier runoff from sum of monthly melt and liquid precipitation on and off the glacier using a fixed-gauge with a glacier minimum reference area from year 2000'}\n",
    "                        ds_all.append(ds_tmp)  # add the dataset with extra coordinates to our final ds_all array\n",
    "\n",
    "                except OSError as err:  # here we land if an error occured\n",
    "                    if str(err) == 'no files to open':  # This is the error message if the GCM, SCENARIO (here ssp) combination does not exist\n",
    "                        print(f'No data for {scen} found!')  # print a descriptive message\n",
    "                    else:\n",
    "                        raise OSError(err)  # if an other error occured we just raise it\n",
    "            # do the actual merging\n",
    "            ds_merged = xr.combine_by_coords(ds_all, fill_value=np.nan)  # define how the missing GCM, SCENARIO combinations should be filled  \n",
    "            #ds_merged.runoff.attrs = {'unit':'kg yr-1',\n",
    "            #             'description':'Annual glacier runoff: sum of annual melt and liquid precipitation on and off the glacier, minimum reference area: 2000'}\n",
    "            #ds_merged.runoff_monthly.attrs = {'unit':'kg month-1',\n",
    "            #             'description':'Monthly glacier runoff: sum of monthly melt and liquid precipitation on and off the glacier, minimum reference area: 2000'}\n",
    "            ds_merged.to_netcdf(f'{path}/basins/{hist}{bc}/{basin_idx}/basin_{basin_idx}_run_hydro_{hist}_endyr2500{bc}.nc')\n",
    "            #ds_merged.to_netcdf(f'{path}/basins/basin_{basin_idx}_combined_run_hydro_{hist}_endyr2100_CMIP6{bc}.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3979572-c96e-43b2-83ae-3b6f1e712009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce520f49-7889-444c-9a8b-bd205ee66e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "    # we choose here the option that is most similar to the one from Dave, i.e. use GCMs from 2000 onwards (but the thing is that Dave also always calibrated to match\n",
    "    # the GCM???, no, or???\n",
    "    #hist = 'gcm_from_2000'\n",
    "    fig,axs = plt.subplots(1,2, figsize=(18,9))\n",
    "    color_dict={'ssp126':'blue', 'ssp370':'orange', 'ssp585':'red'}\n",
    "    for hist,ls in zip(['gcm_from_2000', 'merged'], ['-', '--']):\n",
    "        ds_merged = xr.open_dataset(f'{path}/basins/{basin}_combined_run_hydro_{hist}_endyr2100_CMIP6_bc_2000_2019.nc')\n",
    "        ds_merged = ds_merged.sel(scenario = ['ssp126','ssp245', 'ssp585'])\n",
    "        rgis_working = ds_merged.dropna(dim='rgi_id', how='all').rgi_id.values\n",
    "        ds_merged = ds_merged.sel(rgi_id = rgis_working)\n",
    "        print(len(rgis_working))\n",
    "        ds_merged_sum = ds_merged.sum(dim='rgi_id',  # over which dimension the sum should be taken, here we want to sum up over all glacier ids\n",
    "                                  skipna=True,  # ignore nan values\n",
    "                                  # important, we need values for every glacier (here 2) to do the sum\n",
    "                                  # if some have NaN, the sum should also be NaN (if you don't set min_count, the sum will be 0, which is bad)\n",
    "                                  min_count=len(ds_merged.rgi_id), \n",
    "                                  keep_attrs=True)\n",
    "        ds_merged_sum_med = ds_merged_sum.median(dim='gcm',  # over which dimension the median should be calculated\n",
    "                                                    skipna=True,  # ignore nan values\n",
    "                                                    keep_attrs=True)\n",
    "        \n",
    "        # volume plot\n",
    "        ds_merged_vol_sum_med = ds_merged_sum_med.volume/1e9  # keep all variable descriptions\n",
    "        sns.lineplot(x='time', y='volume (km³)',\n",
    "                     hue='scenario',\n",
    "                 data= ds_merged_vol_sum_med.to_dataframe('volume (km³)'),\n",
    "                    ls = ls, hue_order=['ssp126','ssp245', 'ssp585'], palette=['blue', 'orange', 'red'], ax=axs[0]);\n",
    "        \n",
    "        # runoff plot\n",
    "        # Select the variables relevant for runoff.\n",
    "        # Convert to mega tonnes instead of kg.\n",
    "        runoff_vars= ['melt_off_glacier', 'melt_on_glacier', 'liq_prcp_off_glacier', 'liq_prcp_on_glacier']\n",
    "        #df_runoff = (ds_merged_sum_med[runoff_vars].to_dataframe() * 1e-9).sum(axis=1, min_count=4).to_frame(f'{basin} runoff (MT)')\n",
    "        df_runoff = (ds_merged_sum_med[runoff_vars].to_dataframe()[runoff_vars] * 1e-9).sum(axis=1, min_count=4).to_frame(f'{basin} runoff (MT)')\n",
    "\n",
    "\n",
    "        sns.lineplot(x='time', y=f'{basin} runoff (MT)',\n",
    "                     hue='scenario',\n",
    "                 data= df_runoff,\n",
    "                    ls = ls, hue_order=['ssp126','ssp245', 'ssp585'], palette=['blue', 'orange', 'red'], ax=axs[1]);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
